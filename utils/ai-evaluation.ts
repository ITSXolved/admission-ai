import { GoogleGenAI, createUserContent, createPartFromUri, MediaResolution } from '@google/genai'
import { createClient } from '@supabase/supabase-js'

const supabase = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_ROLE_KEY!
)

export async function evaluateWrittenAnswer(
    attemptId: string,
    questionId: string,
    imageUrls: string[],
    totalMarks: number,
    language: string = 'English'
) {
    console.log(`Evaluating Question ${questionId} for Attempt ${attemptId}`)

    // 1. Prepare Google Gemini
    const apiKey = process.env.GOOGLE_API_KEY
    if (!apiKey) throw new Error('GOOGLE_API_KEY is not defined')

    const genAI = new GoogleGenAI({ apiKey })
    const modelName = process.env.GOOGLE_AI_MODEL || 'gemini-1.5-flash'

    // 2. Fetch Question Text
    const { data: questionData, error: qError } = await supabase
        .from('exam_questions')
        .select('*, question_bank(question_text, answer_key)')
        .eq('id', questionId)
        .single()

    if (qError || !questionData) {
        console.error('Error fetching question data:', qError)
        throw new Error('Question data not found')
    }

    // Check if marks is valid - prioritize DB value
    const validMarks = questionData.marks || totalMarks || 6

    const questionText = questionData.question_bank?.question_text || 'Evaluate the handwritten answer.'
    const answerKey = questionData.question_bank?.answer_key || ''

    // 3. Prepare Image Parts (Upload to File API)
    const imagePartsProm = imageUrls.map(async (url) => {
        try {
            const response = await fetch(url)
            const blob = await response.blob()
            const mimeType = response.headers.get("content-type") || "image/jpeg"

            // Upload to Google GenAI File API
            const uploadResult = await genAI.files.upload({
                file: blob,
                config: { mimeType }
            })

            if (!uploadResult.uri) {
                throw new Error("Upload failed: No URI returned from Google GenAI")
            }

            return createPartFromUri(
                uploadResult.uri,
                uploadResult.mimeType || mimeType
            )
        } catch (e) {
            console.error('Error fetching/uploading image:', url, e)
            return null
        }
    })

    const imageParts = (await Promise.all(imagePartsProm)).filter(p => p !== null) as any[]
    if (imageParts.length === 0) throw new Error('No valid images to evaluate')

    // 4. Construct Prompt
    const promptText = `
    You are an expert strict examiner.
    Question: "${questionText}"
    ${answerKey ? `Answer Key / Model Answer: "${answerKey}"\n    (Use this answer key to verify the correctness of the student's answer. If the student's answer matches the key concepts, award marks accordingly. If it contradicts, deduct marks.)` : ''}
    Maximum Marks for this Question: ${validMarks}
    Language: ${language}

    Task:
    1. Transcribe the handwritten answer from the image(s) exactly.
    2. Evaluate the answer based on the following 5 criteria. 
       CRITICAL: Each criterion must be scored out of ${validMarks} (NOT out of 10).
       - Content Relevance (0-${validMarks})
       - Grammar and Syntax (0-${validMarks})
       - Vocabulary (0-${validMarks})
       - Coherence and Structure (0-${validMarks})
       - Legibility (0-${validMarks})

    Strictly output VALID JSON only:
    {
        "extracted_text": "Full transcription of the handwriting...",
        "feedback": "Concise, constructive feedback for the student...",
        "marks_assigned": 0.0,
        "breakdown": {
            "content_relevance": 0.0,
            "grammar": 0.0,
            "vocabulary": 0.0,
            "coherence": 0.0,
            "legibility": 0.0
        }
    }
    IMPORTANT SCORING RULE: The 'marks_assigned' MUST be the exact mathematical average of the 5 breakdown scores above. Do NOT scale it further.
    Example: If Max Marks is 6, and scores are 5, 4, 5, 4, 3 -> marks_assigned = (5+4+5+4+3)/5 = 4.2
    `

    // 5. Call API
    const result = await genAI.models.generateContent({
        model: modelName,
        contents: [
            createUserContent([promptText, ...imageParts])
        ]
    })

    const text = result.text

    console.log("AI Raw Response:", text) // DEBUG LOG

    if (!text) {
        throw new Error("No text generated by AI")
    }

    // Clean and Parse JSON
    const jsonStr = text.replace(/```json/g, '').replace(/```/g, '').trim()
    let evaluation;
    try {
        evaluation = JSON.parse(jsonStr)
        console.log("Parsed Evaluation:", evaluation) // DEBUG LOG
    } catch (e) {
        console.error("Failed to parse AI response", text)
        throw new Error("Invalid AI response format")
    }

    // 6. Save to Database
    // We need the response ID from student_written_responses
    const { data: responseRecord } = await supabase
        .from('student_written_responses')
        .select('id')
        .eq('attempt_id', attemptId)
        .eq('question_id', questionId)
        .single()

    if (!responseRecord) throw new Error('Student response record not found')

    // Sanitize language for DB enum (assuming 'English' as fallback for 'General' or others)
    // You might want to expand this list based on your actual ENUM values
    const validLanguages = ['English', 'Hindi', 'Malayalam', 'Arabic', 'Urdu', 'Gujarati'] // Example valid values
    const dbLanguage = validLanguages.includes(language) ? language : 'English'

    const dbData = {
        written_response_id: responseRecord.id, // Correct column name
        extracted_text: evaluation.extracted_text,
        feedback: evaluation.feedback,
        total_score: evaluation.marks_assigned,
        // Map breakdown to individual columns
        content_relevance_score: evaluation.breakdown?.content_relevance || 0,
        grammar_score: evaluation.breakdown?.grammar || 0,
        vocabulary_score: evaluation.breakdown?.vocabulary || 0,
        coherence_score: evaluation.breakdown?.coherence || 0,
        legibility_score: evaluation.breakdown?.legibility || 0,
        evaluator_type: 'ai',
        evaluated_at: new Date().toISOString(),
        language: dbLanguage
    }

    const { error: dbError } = await supabase
        .from('written_evaluations')
        .upsert(dbData, { onConflict: 'written_response_id' })

    if (dbError) throw dbError

    return evaluation
}
